{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57236,"databundleVersionId":7292407,"sourceType":"competition"},{"sourceId":7348254,"sourceType":"datasetVersion","datasetId":4266997},{"sourceId":7406639,"sourceType":"datasetVersion","datasetId":4307493},{"sourceId":160094506,"sourceType":"kernelVersion"}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":9754.047629,"end_time":"2024-01-23T11:17:02.547025","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-23T08:34:28.499396","version":"2.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\n\n\nwarnings.filterwarnings(\"ignore\")\n\n\nimport os\nimport gc\nimport pickle\nimport datetime\n\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport joblib\nfrom sklearn.ensemble import VotingRegressor\nimport lightgbm as lgb\nfrom joblib import load\n\n\nimport holidays","metadata":{"papermill":{"duration":4.268374,"end_time":"2024-01-23T08:34:36.205804","exception":false,"start_time":"2024-01-23T08:34:31.93743","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:29:11.260225Z","iopub.execute_input":"2024-01-31T07:29:11.260637Z","iopub.status.idle":"2024-01-31T07:29:14.593995Z","shell.execute_reply.started":"2024-01-31T07:29:11.260606Z","shell.execute_reply":"2024-01-31T07:29:14.593018Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Step 1 : Create a class to access and process all the data files","metadata":{"papermill":{"duration":0.007597,"end_time":"2024-01-23T08:34:36.24016","exception":false,"start_time":"2024-01-23T08:34:36.232563","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class DataStorage:\n    root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n\n    data_cols = [\n        \"target\",\n        \"county\",\n        \"is_business\",\n        \"product_type\",\n        \"is_consumption\",\n        \"datetime\",\n        \"row_id\",\n    ]\n    client_cols = [\n        \"product_type\",\n        \"county\",\n        \"eic_count\",\n        \"installed_capacity\",\n        \"is_business\",\n        \"date\",\n    ]\n    gas_prices_cols = [\"forecast_date\", \"lowest_price_per_mwh\", \"highest_price_per_mwh\"]\n    electricity_prices_cols = [\"forecast_date\", \"euros_per_mwh\"]\n    forecast_weather_cols = [\n        \"latitude\",\n        \"longitude\",\n        \"hours_ahead\",\n        \"temperature\",\n        \"dewpoint\",\n        \"cloudcover_high\",\n        \"cloudcover_low\",\n        \"cloudcover_mid\",\n        \"cloudcover_total\",\n        \"10_metre_u_wind_component\",\n        \"10_metre_v_wind_component\",\n        \"forecast_datetime\",\n        \"direct_solar_radiation\",\n        \"surface_solar_radiation_downwards\",\n        \"snowfall\",\n        \"total_precipitation\",\n    ]\n    historical_weather_cols = [\n        \"datetime\",\n        \"temperature\",\n        \"dewpoint\",\n        \"rain\",\n        \"snowfall\",\n        \"surface_pressure\",\n        \"cloudcover_total\",\n        \"cloudcover_low\",\n        \"cloudcover_mid\",\n        \"cloudcover_high\",\n        \"windspeed_10m\",\n        \"winddirection_10m\",\n        \"shortwave_radiation\",\n        \"direct_solar_radiation\",\n        \"diffuse_radiation\",\n        \"latitude\",\n        \"longitude\",\n    ]\n    location_cols = [\"longitude\", \"latitude\", \"county\"]\n    target_cols = [\n        \"target\",\n        \"county\",\n        \"is_business\",\n        \"product_type\",\n        \"is_consumption\",\n        \"datetime\",\n    ]\n\n    def __init__(self):\n        self.df_data = pl.read_csv(\n            os.path.join(self.root, \"train.csv\"),\n            columns=self.data_cols,\n            try_parse_dates=True,\n        )\n        self.df_client = pl.read_csv(\n            os.path.join(self.root, \"client.csv\"),\n            columns=self.client_cols,\n            try_parse_dates=True,\n        )\n        self.df_gas_prices = pl.read_csv(\n            os.path.join(self.root, \"gas_prices.csv\"),\n            columns=self.gas_prices_cols,\n            try_parse_dates=True,\n        )\n        self.df_electricity_prices = pl.read_csv(\n            os.path.join(self.root, \"electricity_prices.csv\"),\n            columns=self.electricity_prices_cols,\n            try_parse_dates=True,\n        )\n        self.df_forecast_weather = pl.read_csv(\n            os.path.join(self.root, \"forecast_weather.csv\"),\n            columns=self.forecast_weather_cols,\n            try_parse_dates=True,\n        )\n        self.df_historical_weather = pl.read_csv(\n            os.path.join(self.root, \"historical_weather.csv\"),\n            columns=self.historical_weather_cols,\n            try_parse_dates=True,\n        )\n        self.df_weather_station_to_county_mapping = pl.read_csv(\n            os.path.join(self.root, \"weather_station_to_county_mapping.csv\"),\n            columns=self.location_cols,\n            try_parse_dates=True,\n        )\n        self.df_data = self.df_data.filter(\n            pl.col(\"datetime\") >= pd.to_datetime(\"2022-01-01\")\n        )\n        self.df_target = self.df_data.select(self.target_cols)\n\n        self.schema_data = self.df_data.schema\n        self.schema_client = self.df_client.schema\n        self.schema_gas_prices = self.df_gas_prices.schema\n        self.schema_electricity_prices = self.df_electricity_prices.schema\n        self.schema_forecast_weather = self.df_forecast_weather.schema\n        self.schema_historical_weather = self.df_historical_weather.schema\n        self.schema_target = self.df_target.schema\n\n        self.df_weather_station_to_county_mapping = (\n            self.df_weather_station_to_county_mapping.with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            )\n        )\n\n    def update_with_new_data(\n        self,\n        df_new_client,\n        df_new_gas_prices,\n        df_new_electricity_prices,\n        df_new_forecast_weather,\n        df_new_historical_weather,\n        df_new_target,\n    ):\n        df_new_client = pl.from_pandas(\n            df_new_client[self.client_cols], schema_overrides=self.schema_client\n        )\n        df_new_gas_prices = pl.from_pandas(\n            df_new_gas_prices[self.gas_prices_cols],\n            schema_overrides=self.schema_gas_prices,\n        )\n        df_new_electricity_prices = pl.from_pandas(\n            df_new_electricity_prices[self.electricity_prices_cols],\n            schema_overrides=self.schema_electricity_prices,\n        )\n        df_new_forecast_weather = pl.from_pandas(\n            df_new_forecast_weather[self.forecast_weather_cols],\n            schema_overrides=self.schema_forecast_weather,\n        )\n        df_new_historical_weather = pl.from_pandas(\n            df_new_historical_weather[self.historical_weather_cols],\n            schema_overrides=self.schema_historical_weather,\n        )\n        df_new_target = pl.from_pandas(\n            df_new_target[self.target_cols], schema_overrides=self.schema_target\n        )\n\n        self.df_client = pl.concat([self.df_client, df_new_client]).unique(\n            [\"date\", \"county\", \"is_business\", \"product_type\"]\n        )\n        self.df_gas_prices = pl.concat([self.df_gas_prices, df_new_gas_prices]).unique(\n            [\"forecast_date\"]\n        )\n        self.df_electricity_prices = pl.concat(\n            [self.df_electricity_prices, df_new_electricity_prices]\n        ).unique([\"forecast_date\"])\n        self.df_forecast_weather = pl.concat(\n            [self.df_forecast_weather, df_new_forecast_weather]\n        ).unique([\"forecast_datetime\", \"latitude\", \"longitude\", \"hours_ahead\"])\n        self.df_historical_weather = pl.concat(\n            [self.df_historical_weather, df_new_historical_weather]\n        ).unique([\"datetime\", \"latitude\", \"longitude\"])\n        self.df_target = pl.concat([self.df_target, df_new_target]).unique(\n            [\"datetime\", \"county\", \"is_business\", \"product_type\", \"is_consumption\"]\n        )\n\n    def preprocess_test(self, df_test):\n        df_test = df_test.rename(columns={\"prediction_datetime\": \"datetime\"})\n        df_test = pl.from_pandas(\n            df_test[self.data_cols[1:]], schema_overrides=self.schema_data\n        )\n        return df_test\n","metadata":{"papermill":{"duration":0.033711,"end_time":"2024-01-23T08:34:36.281879","exception":false,"start_time":"2024-01-23T08:34:36.248168","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:29:14.596258Z","iopub.execute_input":"2024-01-31T07:29:14.596714Z","iopub.status.idle":"2024-01-31T07:29:14.625613Z","shell.execute_reply.started":"2024-01-31T07:29:14.596684Z","shell.execute_reply":"2024-01-31T07:29:14.624198Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Step 2: Create Feature Enegineering Classes","metadata":{"papermill":{"duration":0.00766,"end_time":"2024-01-23T08:34:36.29774","exception":false,"start_time":"2024-01-23T08:34:36.29008","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class FeatureEngineer:\n    def __init__(self, data):\n        self.data = data\n        self.estonian_holidays = list(\n            holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n        )\n\n        \n    def _general_features(self, df_features):\n        df_features = (\n            df_features.with_columns(\n                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n                pl.col(\"datetime\").dt.day().alias(\"day\"),pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n                pl.col(\"datetime\").dt.month().alias(\"month\"),pl.col(\"datetime\").dt.year().alias(\"year\"),\n            )\n            .with_columns(pl.concat_str(\"county\",\"is_business\",\"product_type\",\"is_consumption\",separator=\"_\",).alias(\"segment\"),)\n            .with_columns(\n                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),(np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),(np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n            )\n        )\n        return df_features\n\n    def _client_features(self, df_features):\n        df_client = self.data.df_client\n\n        df_features = df_features.join(\n            df_client.with_columns((pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n            ),on=[\"county\", \"is_business\", \"product_type\", \"date\"],how=\"left\",\n        )\n        return df_features\n    \n    def is_country_holiday(self, row):\n        return (\n            datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])\n            in self.estonian_holidays\n        )\n\n    def _holidays_features(self, df_features):\n        df_features = df_features.with_columns(\n            pl.struct([\"year\", \"month\", \"day\"])\n            .apply(self.is_country_holiday)\n            .alias(\"is_country_holiday\")\n        )\n        return df_features\n\n    def _forecast_weather_features(self, df_features):\n        df_forecast_weather = self.data.df_forecast_weather\n        df_weather_station_to_county_mapping = (\n            self.data.df_weather_station_to_county_mapping\n        )\n\n        df_forecast_weather = (\n            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n#             .drop(\"hours_ahead\")\n            .with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),)\n            .join(df_weather_station_to_county_mapping,how=\"left\",on=[\"longitude\", \"latitude\"],).drop(\"longitude\", \"latitude\"))\n\n        df_forecast_weather_date = (df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\"))\n\n        df_forecast_weather_local = (df_forecast_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean())\n\n        for hours_lag in [0, 7 * 24]:\n            df_features = df_features.join(\n                df_forecast_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),on=\"datetime\",how=\"left\",suffix=f\"_forecast_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_forecast_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),on=[\"county\", \"datetime\"],how=\"left\",suffix=f\"_forecast_local_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _historical_weather_features(self, df_features):\n        df_historical_weather = self.data.df_historical_weather\n        df_weather_station_to_county_mapping = (self.data.df_weather_station_to_county_mapping)\n\n        df_historical_weather = (\n            df_historical_weather.with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            ).join(df_weather_station_to_county_mapping,how=\"left\",on=[\"longitude\", \"latitude\"],).drop(\"longitude\", \"latitude\")\n        )\n\n        df_historical_weather_date = (df_historical_weather.group_by(\"datetime\").mean().drop(\"county\"))\n\n        df_historical_weather_local = (\n            df_historical_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean()\n        )\n\n        for hours_lag in [2 * 24, 7 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),on=\"datetime\",how=\"left\",suffix=f\"_historical_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_historical_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),on=[\"county\", \"datetime\"],how=\"left\",suffix=f\"_historical_local_{hours_lag}h\",\n            )\n\n        for hours_lag in [1 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),).filter(pl.col(\"hour\") <= 10).drop(\"hour\"),on=\"datetime\",how=\"left\",\n                suffix=f\"_historical_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _target_features(self, df_features):\n        df_target = self.data.df_target\n\n        df_target_all_type_sum = (df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\"))\n\n        df_target_all_county_type_sum = (df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\", \"county\"))\n        \n        hours_list=[i*24 for i in range(2,15)]\n\n        for hours_lag in hours_list:\n            df_features = df_features.join(\n                df_target.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n                on=[\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",],\n                how=\"left\",\n            )\n\n        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n            df_features = df_features.join(\n                df_target_all_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],how=\"left\",\n            )\n\n            df_features = df_features.join(\n                df_target_all_county_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n                on=[\"is_business\", \"is_consumption\", \"datetime\"],how=\"left\",\n                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n            )\n\n        cols_for_stats = [f\"target_{hours_lag}h\" for hours_lag in hours_list[:4]]\n        \n        df_features = df_features.with_columns(\n            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n            df_features.select(cols_for_stats).transpose().std().transpose().to_series().alias(f\"target_std\"),\n            )\n\n        for target_prefix, lag_nominator, lag_denomonator in [\n            (\"target\", 24 * 7, 24 * 14),(\"target\", 24 * 2, 24 * 9),(\"target\", 24 * 3, 24 * 10),(\"target\", 24 * 2, 24 * 3),\n            (\"target_all_type_sum\", 24 * 2, 24 * 3),(\"target_all_type_sum\", 24 * 7, 24 * 14),\n            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),(\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n        ]:\n            df_features = df_features.with_columns(\n                (pl.col(f\"{target_prefix}_{lag_nominator}h\")/ (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n            )\n\n        return df_features\n\n    def _reduce_memory_usage(self, df_features):\n        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n        return df_features\n\n    def _drop_columns(self, df_features):\n        df_features = df_features.drop(\"datetime\", \"hour\", \"dayofyear\")\n        return df_features\n\n    def _to_pandas(self, df_features, y):\n        cat_cols = [\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"segment\",]\n\n        if y is not None:\n            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n        else:\n            df_features = df_features.to_pandas()\n\n        df_features = df_features.set_index(\"row_id\")\n        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n\n        return df_features\n    \n    # added some new features here\n    def _additional_features(self,df):\n        for col in [\n                    'temperature', \n                    'dewpoint', \n                    '10_metre_u_wind_component', \n                    '10_metre_v_wind_component', \n            ]:\n            for window in [1]:\n                df[f\"{col}_diff_{window}\"] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])[col].diff(window)\n        return df\n    \n    def _log_outliers(self,df):\n        l1=['installed_capacity', 'target_mean', 'target_std']\n        for i in l1:\n            df = df.with_columns([(f\"log_{i}\", pl.when(df[i] != 0).then(np.log(pl.col(i))).otherwise(0))])\n        return df\n        \n\n    def generate_features(self, df_prediction_items,isTrain):\n        if \"target\" in df_prediction_items.columns:\n            df_prediction_items, y = (\n                df_prediction_items.drop(\"target\"),\n                df_prediction_items.select(\"target\"),\n            )\n        else:\n            y = None\n\n        df_features = df_prediction_items.with_columns(\n            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n        )\n\n        for add_features in [\n            self._general_features,self._client_features,self._forecast_weather_features,\n            self._historical_weather_features,self._target_features,self._holidays_features,\n            self._log_outliers,self._reduce_memory_usage,self._drop_columns,]:\n            df_features = add_features(df_features)\n\n        df_features = self._to_pandas(df_features, y)\n        df_features = self._additional_features(df_features)\n\n        return df_features\n","metadata":{"papermill":{"duration":0.05176,"end_time":"2024-01-23T08:34:36.377413","exception":false,"start_time":"2024-01-23T08:34:36.325653","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:29:14.630682Z","iopub.execute_input":"2024-01-31T07:29:14.631202Z","iopub.status.idle":"2024-01-31T07:29:14.681257Z","shell.execute_reply.started":"2024-01-31T07:29:14.631169Z","shell.execute_reply":"2024-01-31T07:29:14.680017Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class FeaturesGenerator:\n    def __init__(self, data_storage):\n        self.data_storage = data_storage\n        self.estonian_holidays = list(\n            holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n        )\n\n    def _add_general_features(self, df_features):\n        df_features = (\n            df_features.with_columns(\n                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n                pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n                pl.col(\"datetime\").dt.day().alias(\"day\"),\n                pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n                pl.col(\"datetime\").dt.month().alias(\"month\"),\n                pl.col(\"datetime\").dt.year().alias(\"year\"),\n            )\n            .with_columns(\n                pl.concat_str(\n                    \"county\",\n                    \"is_business\",\n                    \"product_type\",\n                    \"is_consumption\",\n                    separator=\"_\",\n                ).alias(\"segment\"),\n            )\n            .with_columns(\n                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n                (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n                (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n            )\n        )\n        return df_features\n\n    \n    def _add_client_features(self, df_features):\n        df_client = self.data_storage.df_client\n\n        df_features = df_features.join(\n            df_client.with_columns(\n                (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n            ),\n            on=[\"county\", \"is_business\", \"product_type\", \"date\"],\n            how=\"left\",\n        )\n        return df_features\n    \n    def is_country_holiday(self, row):\n        return (\n            datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])\n            in self.estonian_holidays\n        )\n\n    def _add_holidays_features(self, df_features):\n        df_features = df_features.with_columns(\n            pl.struct([\"year\", \"month\", \"day\"])\n            .apply(self.is_country_holiday)\n            .alias(\"is_country_holiday\")\n        )\n        return df_features\n\n    def _add_forecast_weather_features(self, df_features):\n        df_forecast_weather = self.data_storage.df_forecast_weather\n        df_weather_station_to_county_mapping = (\n            self.data_storage.df_weather_station_to_county_mapping\n        )\n\n        df_forecast_weather = (\n            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n            .drop(\"hours_ahead\")\n            .with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            )\n            .join(\n                df_weather_station_to_county_mapping,\n                how=\"left\",\n                on=[\"longitude\", \"latitude\"],\n            )\n            .drop(\"longitude\", \"latitude\")\n        )\n\n        df_forecast_weather_date = (\n            df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\")\n        )\n\n        df_forecast_weather_local = (\n            df_forecast_weather.filter(pl.col(\"county\").is_not_null())\n            .group_by(\"county\", \"datetime\")\n            .mean()\n        )\n\n        for hours_lag in [0, 7 * 24]:\n            df_features = df_features.join(\n                df_forecast_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_forecast_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_forecast_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=[\"county\", \"datetime\"],\n                how=\"left\",\n                suffix=f\"_forecast_local_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _add_historical_weather_features(self, df_features):\n        df_historical_weather = self.data_storage.df_historical_weather\n        df_weather_station_to_county_mapping = (\n            self.data_storage.df_weather_station_to_county_mapping\n        )\n\n        df_historical_weather = (\n            df_historical_weather.with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            )\n            .join(\n                df_weather_station_to_county_mapping,\n                how=\"left\",\n                on=[\"longitude\", \"latitude\"],\n            )\n            .drop(\"longitude\", \"latitude\")\n        )\n\n        df_historical_weather_date = (\n            df_historical_weather.group_by(\"datetime\").mean().drop(\"county\")\n        )\n\n        df_historical_weather_local = (\n            df_historical_weather.filter(pl.col(\"county\").is_not_null())\n            .group_by(\"county\", \"datetime\")\n            .mean()\n        )\n\n        for hours_lag in [2 * 24, 7 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_historical_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_historical_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=[\"county\", \"datetime\"],\n                how=\"left\",\n                suffix=f\"_historical_local_{hours_lag}h\",\n            )\n\n        for hours_lag in [1 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n                )\n                .filter(pl.col(\"hour\") <= 10)\n                .drop(\"hour\"),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_historical_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _add_target_features(self, df_features):\n        df_target = self.data_storage.df_target\n\n        df_target_all_type_sum = (\n            df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"])\n            .sum()\n            .drop(\"product_type\")\n        )\n\n        df_target_all_county_type_sum = (\n            df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"])\n            .sum()\n            .drop(\"product_type\", \"county\")\n        )\n\n        for hours_lag in [2 * 24,3 * 24,4 * 24,5 * 24,6 * 24,7 * 24,8 * 24,9 * 24,10 * 24,11 * 24,12 * 24,13 * 24,14 * 24,]:\n            df_features = df_features.join(\n                df_target.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n                on=[\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",],\n                how=\"left\",\n            )\n\n        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n            df_features = df_features.join(\n                df_target_all_type_sum.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],\n                how=\"left\",\n            )\n\n            df_features = df_features.join(\n                df_target_all_county_type_sum.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n                on=[\"is_business\", \"is_consumption\", \"datetime\"],\n                how=\"left\",\n                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n            )\n\n        cols_for_stats = [\n            f\"target_{hours_lag}h\" for hours_lag in [2 * 24, 3 * 24, 4 * 24, 5 * 24]\n        ]\n        df_features = df_features.with_columns(\n            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n            df_features.select(cols_for_stats)\n            .transpose()\n            .std()\n            .transpose()\n            .to_series()\n            .alias(f\"target_std\"),\n        )\n\n        for target_prefix, lag_nominator, lag_denomonator in [\n            (\"target\", 24 * 7, 24 * 14),\n            (\"target\", 24 * 2, 24 * 9),\n            (\"target\", 24 * 3, 24 * 10),\n            (\"target\", 24 * 2, 24 * 3),\n            (\"target_all_type_sum\", 24 * 2, 24 * 3),\n            (\"target_all_type_sum\", 24 * 7, 24 * 14),\n            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),\n            (\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n        ]:\n            df_features = df_features.with_columns(\n                (\n                    pl.col(f\"{target_prefix}_{lag_nominator}h\")\n                    / (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n            )\n\n        return df_features\n\n    def _reduce_memory_usage(self, df_features):\n        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n        return df_features\n\n    def _drop_columns(self, df_features):\n        df_features = df_features.drop(\n            \"date\", \"datetime\", \"hour\", \"dayofyear\"\n        )\n        return df_features\n\n    def _to_pandas(self, df_features, y):\n        cat_cols = [\n            \"county\",\n            \"is_business\",\n            \"product_type\",\n            \"is_consumption\",\n            \"segment\",\n        ]\n\n        if y is not None:\n            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n        else:\n            df_features = df_features.to_pandas()\n\n        df_features = df_features.set_index(\"row_id\")\n        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n\n        return df_features\n    \n    def generate_features(self, df_prediction_items):\n        if \"target\" in df_prediction_items.columns:\n            df_prediction_items, y = (\n                df_prediction_items.drop(\"target\"),\n                df_prediction_items.select(\"target\"),\n            )\n        else:\n            y = None\n\n        df_features = df_prediction_items.with_columns(\n            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n        )\n\n        for add_features in [\n            self._add_general_features,\n            self._add_client_features,\n            self._add_forecast_weather_features,\n            self._add_historical_weather_features,\n            self._add_target_features,\n            self._add_holidays_features,\n            self._reduce_memory_usage,\n            self._drop_columns,\n        ]:\n            df_features = add_features(df_features)\n\n        df_features = self._to_pandas(df_features, y)\n\n        return df_features","metadata":{"papermill":{"duration":0.050763,"end_time":"2024-01-23T08:34:36.43643","exception":false,"start_time":"2024-01-23T08:34:36.385667","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:29:14.683392Z","iopub.execute_input":"2024-01-31T07:29:14.683899Z","iopub.status.idle":"2024-01-31T07:29:14.735781Z","shell.execute_reply.started":"2024-01-31T07:29:14.683839Z","shell.execute_reply":"2024-01-31T07:29:14.734454Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Initialisation","metadata":{"papermill":{"duration":0.007603,"end_time":"2024-01-23T08:34:36.45225","exception":false,"start_time":"2024-01-23T08:34:36.444647","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data_storage = DataStorage()\nfeatures_generator = FeaturesGenerator(data_storage=data_storage)\nfeat_gen = FeatureEngineer(data=data_storage)","metadata":{"papermill":{"duration":5.572439,"end_time":"2024-01-23T08:34:42.032717","exception":false,"start_time":"2024-01-23T08:34:36.460278","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:29:14.739572Z","iopub.execute_input":"2024-01-31T07:29:14.739968Z","iopub.status.idle":"2024-01-31T07:29:20.714171Z","shell.execute_reply.started":"2024-01-31T07:29:14.739936Z","shell.execute_reply":"2024-01-31T07:29:20.713174Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Feature Generation","metadata":{"papermill":{"duration":0.007548,"end_time":"2024-01-23T08:34:42.048275","exception":false,"start_time":"2024-01-23T08:34:42.040727","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Although there is no requirement as such to create the training datasets, (since we are not doing any model training here) I have created them only to show the difference in the features between the two datasets we are using here\n\ndf_train_features = features_generator.generate_features(data_storage.df_data)\ndf_train_features = df_train_features[df_train_features['target'].notnull()]","metadata":{"papermill":{"duration":25.545725,"end_time":"2024-01-23T08:35:07.617607","exception":false,"start_time":"2024-01-23T08:34:42.071882","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:29:20.715396Z","iopub.execute_input":"2024-01-31T07:29:20.715708Z","iopub.status.idle":"2024-01-31T07:29:49.912551Z","shell.execute_reply.started":"2024-01-31T07:29:20.715681Z","shell.execute_reply":"2024-01-31T07:29:49.911190Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_train = feat_gen.generate_features(data_storage.df_data,True)\ndf_train = df_train[df_train['target'].notnull()]","metadata":{"papermill":{"duration":25.817152,"end_time":"2024-01-23T08:35:33.442902","exception":false,"start_time":"2024-01-23T08:35:07.62575","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:29:49.913987Z","iopub.execute_input":"2024-01-31T07:29:49.914545Z","iopub.status.idle":"2024-01-31T07:30:19.029141Z","shell.execute_reply.started":"2024-01-31T07:29:49.914506Z","shell.execute_reply":"2024-01-31T07:30:19.027322Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_train_features.shape","metadata":{"papermill":{"duration":0.01941,"end_time":"2024-01-23T08:35:33.470474","exception":false,"start_time":"2024-01-23T08:35:33.451064","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:30:19.032544Z","iopub.execute_input":"2024-01-31T07:30:19.033208Z","iopub.status.idle":"2024-01-31T07:30:19.045096Z","shell.execute_reply.started":"2024-01-31T07:30:19.033161Z","shell.execute_reply":"2024-01-31T07:30:19.043676Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(1651902, 166)"},"metadata":{}}]},{"cell_type":"code","source":"df_train.shape","metadata":{"papermill":{"duration":0.019303,"end_time":"2024-01-23T08:35:33.498287","exception":false,"start_time":"2024-01-23T08:35:33.478984","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:30:19.046755Z","iopub.execute_input":"2024-01-31T07:30:19.047305Z","iopub.status.idle":"2024-01-31T07:30:19.061996Z","shell.execute_reply.started":"2024-01-31T07:30:19.047273Z","shell.execute_reply":"2024-01-31T07:30:19.060723Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(1651902, 176)"},"metadata":{}}]},{"cell_type":"code","source":"gc.collect()","metadata":{"papermill":{"duration":0.122199,"end_time":"2024-01-23T08:35:33.628954","exception":false,"start_time":"2024-01-23T08:35:33.506755","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:30:19.063768Z","iopub.execute_input":"2024-01-31T07:30:19.064114Z","iopub.status.idle":"2024-01-31T07:30:19.213508Z","shell.execute_reply.started":"2024-01-31T07:30:19.064083Z","shell.execute_reply":"2024-01-31T07:30:19.212186Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"for col in df_train.columns:\n    if (col not in df_train_features.columns):\n        print(col)","metadata":{"papermill":{"duration":0.017988,"end_time":"2024-01-23T08:35:33.655284","exception":false,"start_time":"2024-01-23T08:35:33.637296","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:30:19.214879Z","iopub.execute_input":"2024-01-31T07:30:19.215268Z","iopub.status.idle":"2024-01-31T07:30:19.227994Z","shell.execute_reply.started":"2024-01-31T07:30:19.215237Z","shell.execute_reply":"2024-01-31T07:30:19.226713Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"date\nhours_ahead\nhours_ahead_forecast_local_0h\nhours_ahead_forecast_168h\nhours_ahead_forecast_local_168h\nliteral\ntemperature_diff_1\ndewpoint_diff_1\n10_metre_u_wind_component_diff_1\n10_metre_v_wind_component_diff_1\n","output_type":"stream"}]},{"cell_type":"code","source":"if('date' in df_train_features.columns):\n    df_train_features.drop(columns=['date'], inplace=True)","metadata":{"papermill":{"duration":0.017062,"end_time":"2024-01-23T08:35:33.680724","exception":false,"start_time":"2024-01-23T08:35:33.663662","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:30:19.229359Z","iopub.execute_input":"2024-01-31T07:30:19.229673Z","iopub.status.idle":"2024-01-31T07:30:19.238553Z","shell.execute_reply.started":"2024-01-31T07:30:19.229646Z","shell.execute_reply":"2024-01-31T07:30:19.237184Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## FFT Analysis","metadata":{"papermill":{"duration":0.008163,"end_time":"2024-01-23T08:35:33.697389","exception":false,"start_time":"2024-01-23T08:35:33.689226","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Loading Trained Models","metadata":{"papermill":{"duration":0.008071,"end_time":"2024-01-23T08:35:33.730348","exception":false,"start_time":"2024-01-23T08:35:33.722277","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c1 = load('/kaggle/input/enefit-trained-model/voting_regressor_consumption_model.joblib')\np1 = load('/kaggle/input/enefit-trained-model/voting_regressor_production_model.joblib')\n\ngc.collect()","metadata":{"papermill":{"duration":49.487025,"end_time":"2024-01-23T08:36:23.225767","exception":false,"start_time":"2024-01-23T08:35:33.738742","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:30:19.240265Z","iopub.execute_input":"2024-01-31T07:30:19.240581Z","iopub.status.idle":"2024-01-31T07:31:27.844528Z","shell.execute_reply.started":"2024-01-31T07:30:19.240554Z","shell.execute_reply":"2024-01-31T07:31:27.843157Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"dc1 = load('/kaggle/input/v2-enefit-pebop-eda-plotly-and-modelling/voting_regressor_consumption_model.joblib')\ndp1 = load('/kaggle/input/v2-enefit-pebop-eda-plotly-and-modelling/voting_regressor_production_model.joblib')\ngc.collect()","metadata":{"papermill":{"duration":57.370094,"end_time":"2024-01-23T08:37:20.604381","exception":false,"start_time":"2024-01-23T08:36:23.234287","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:31:27.850333Z","iopub.execute_input":"2024-01-31T07:31:27.850942Z","iopub.status.idle":"2024-01-31T07:32:29.189709Z","shell.execute_reply.started":"2024-01-31T07:31:27.850895Z","shell.execute_reply":"2024-01-31T07:32:29.188303Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"c1.estimators[0]","metadata":{"papermill":{"duration":0.022166,"end_time":"2024-01-23T08:37:20.635262","exception":false,"start_time":"2024-01-23T08:37:20.613096","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:32:29.191474Z","iopub.execute_input":"2024-01-31T07:32:29.191915Z","iopub.status.idle":"2024-01-31T07:32:29.204595Z","shell.execute_reply.started":"2024-01-31T07:32:29.191864Z","shell.execute_reply":"2024-01-31T07:32:29.203295Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"('consumption_lgb_0',\n LGBMRegressor(colsample_bynode=0.548, colsample_bytree=0.878, device='gpu',\n               lambda_l1=3.438, lambda_l2=1.438, learning_rate=0.042,\n               max_depth=17, min_data_in_leaf=50, n_estimators=2500,\n               num_leaves=480, objective='regression_l1', random_state=0))"},"metadata":{}}]},{"cell_type":"markdown","source":"## Declaring separate prediction functions for both models","metadata":{"papermill":{"duration":0.008856,"end_time":"2024-01-23T08:37:20.653153","exception":false,"start_time":"2024-01-23T08:37:20.644297","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from catboost import CatBoostRegressor\n\n\ndef preprocess_cat(df):\n    df.county = df.county.astype(int)\n    df.is_business = df.is_business.astype(int)\n    df.product_type = df.product_type.astype(int)\n    df.is_consumption = df.is_consumption.astype(int)\n    df.segment = df.segment.astype(int)\n\ndef fit_cat(_df):\n    \n    df = _df.copy()\n    task_type = \"CPU\"\n    cat = CatBoostRegressor( colsample_bylevel=0.878, task_type=task_type,\n                    reg_lambda=3.438, learning_rate=0.042,\n                   max_depth=10, min_data_in_leaf=50,\n                    n_estimators=2500,\n                            verbose=1,\n                   objective='MAE', random_state=1)\n    df.county = df.county.astype(int)\n    df.is_business = df.is_business.astype(int)\n    df.product_type = df.product_type.astype(int)\n    df.is_consumption = df.is_consumption.astype(int)\n    preprocess_cat(df)\n    cat.fit(df.drop(\"target\", axis=1), df[\"target\"])\n    del df\n    gc.collect()\n    return cat\n\ndef predict_cat(_df, cat):\n    df = _df.copy()\n    preprocess_cat(df)\n    prediction = cat.predict(df)\n    del df\n    gc.collect()\n    return prediction\n\ndef load_cat():\n    with open(\"/kaggle/input/fork-of-enefit-pebop-disable-plot/cat.pickle\", \"rb\") as f:\n        return pickle.load(f)\n\ncat = load_cat()","metadata":{"papermill":{"duration":9388.550297,"end_time":"2024-01-23T11:13:49.212634","exception":false,"start_time":"2024-01-23T08:37:20.662337","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:32:29.206300Z","iopub.execute_input":"2024-01-31T07:32:29.206733Z","iopub.status.idle":"2024-01-31T07:32:30.226349Z","shell.execute_reply.started":"2024-01-31T07:32:29.206694Z","shell.execute_reply":"2024-01-31T07:32:30.225079Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"with open(\"cat.pickle\", \"wb\") as f:\n    pickle.dump(cat, f)","metadata":{"papermill":{"duration":0.42452,"end_time":"2024-01-23T11:13:49.809324","exception":false,"start_time":"2024-01-23T11:13:49.384804","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:32:30.227792Z","iopub.execute_input":"2024-01-31T07:32:30.228112Z","iopub.status.idle":"2024-01-31T07:32:30.403981Z","shell.execute_reply.started":"2024-01-31T07:32:30.228084Z","shell.execute_reply":"2024-01-31T07:32:30.402919Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def predict(df_features,model_consumption=c1,model_production=p1):\n    predictions = np.zeros(len(df_features))\n\n    mask = df_features[\"is_consumption\"] == 1\n    predictions[mask.values] = (\n        0.8*model_consumption.predict(df_features[mask]).clip(0) +\n        0.2*predict_cat(df_test_features[mask], cat).clip(0))\n\n    mask = df_features[\"is_consumption\"] == 0\n    predictions[mask.values] = (\n        0.8*model_production.predict(df_features[mask]).clip(0) +\n        0.2*predict_cat(df_test_features[mask], cat).clip(0)\n    )\n\n    return predictions","metadata":{"papermill":{"duration":0.178634,"end_time":"2024-01-23T11:13:50.155719","exception":false,"start_time":"2024-01-23T11:13:49.977085","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:32:30.405860Z","iopub.execute_input":"2024-01-31T07:32:30.406247Z","iopub.status.idle":"2024-01-31T07:32:30.418432Z","shell.execute_reply.started":"2024-01-31T07:32:30.406215Z","shell.execute_reply":"2024-01-31T07:32:30.416821Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_model(df_features,hours_lag=48,model_consumption=dc1,model_production=dp1):\n    predictions = np.zeros(len(df_features))\n\n    \n    mask = df_features[\"is_consumption\"] == 1\n    predictions[mask.values] = np.clip(\n        df_features[mask][f\"target_{hours_lag}h\"].fillna(0).values+ \n        model_consumption.predict(df_features[mask]),0,np.inf,\n        )\n\n    mask = df_features[\"is_consumption\"] == 0\n    predictions[mask.values] = np.clip(\n        df_features[mask][f\"target_{hours_lag}h\"].fillna(0).values+ \n        model_production.predict(df_features[mask]),0,np.inf,\n        )\n\n    return predictions\n","metadata":{"papermill":{"duration":0.180143,"end_time":"2024-01-23T11:13:50.50247","exception":false,"start_time":"2024-01-23T11:13:50.322327","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:32:30.420322Z","iopub.execute_input":"2024-01-31T07:32:30.420711Z","iopub.status.idle":"2024-01-31T07:32:30.429422Z","shell.execute_reply.started":"2024-01-31T07:32:30.420668Z","shell.execute_reply":"2024-01-31T07:32:30.428185Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Submit API","metadata":{"papermill":{"duration":0.168435,"end_time":"2024-01-23T11:13:50.926816","exception":false,"start_time":"2024-01-23T11:13:50.758381","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import enefit\n\nenv = enefit.make_env()\niter_test = env.iter_test()","metadata":{"papermill":{"duration":0.212112,"end_time":"2024-01-23T11:13:51.306689","exception":false,"start_time":"2024-01-23T11:13:51.094577","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:32:30.431185Z","iopub.execute_input":"2024-01-31T07:32:30.431626Z","iopub.status.idle":"2024-01-31T07:32:30.473736Z","shell.execute_reply.started":"2024-01-31T07:32:30.431585Z","shell.execute_reply":"2024-01-31T07:32:30.472547Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"for (\n    df_test, \n    df_new_target, \n    df_new_client, \n    df_new_historical_weather,\n    df_new_forecast_weather, \n    df_new_electricity_prices, \n    df_new_gas_prices, \n    df_sample_prediction\n) in iter_test:\n\n    data_storage.update_with_new_data(\n        df_new_client=df_new_client,\n        df_new_gas_prices=df_new_gas_prices,\n        df_new_electricity_prices=df_new_electricity_prices,\n        df_new_forecast_weather=df_new_forecast_weather,\n        df_new_historical_weather=df_new_historical_weather,\n        df_new_target=df_new_target\n    )\n    \n    #separately generate test features for both models\n    \n    df_test = data_storage.preprocess_test(df_test)\n    \n    df_test_features = features_generator.generate_features(df_test)\n    \n    df_test_feats = feat_gen.generate_features(df_test,False)\n    \n    df_test_feats.drop(columns=['date','literal'],inplace=True)\n        \n    pred1 = predict(df_test_features)\n    \n    pred2 = predict_model(df_test_feats)\n    \n    # Ensembling with slightly tuned model weights\n    pred_1_w = 0.49\n    df_sample_prediction[\"target\"] = (\n        (pred_1_w * pred1) + \n        ((1 - pred_1_w) * pred2)\n    )\n    \n    env.predict(df_sample_prediction)\n    gc.collect()","metadata":{"papermill":{"duration":187.85854,"end_time":"2024-01-23T11:16:59.333765","exception":false,"start_time":"2024-01-23T11:13:51.475225","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-31T07:32:30.475814Z","iopub.execute_input":"2024-01-31T07:32:30.476353Z","iopub.status.idle":"2024-01-31T07:35:20.418866Z","shell.execute_reply.started":"2024-01-31T07:32:30.476309Z","shell.execute_reply":"2024-01-31T07:35:20.417642Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}